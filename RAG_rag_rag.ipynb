{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUsTRMJT9zPYcDE71ZSCZk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ammad12345390/RAG_Chatbot-/blob/main/RAG_rag_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKP39oozkACp",
        "outputId": "a34d3cb4-6476-47aa-b57e-892cb0041247"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.32-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.9-py3-none-any.whl.metadata (529 bytes)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.101.0)\n",
            "Collecting openai\n",
            "  Downloading openai-1.102.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting langchain-core<2.0.0,>=0.3.75 (from langchain-community)\n",
            "  Downloading langchain_core-0.3.75-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.43)\n",
            "Collecting requests<3,>=2.32.5 (from langchain-community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.16)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (1.33)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain-community) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.3.29-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.32-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.0.0-py3-none-any.whl (310 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docx2txt-0.9-py3-none-any.whl (4.0 kB)\n",
            "Downloading openai-1.102.0-py3-none-any.whl (812 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m812.0/812.0 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_core-0.3.75-py3-none-any.whl (443 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m444.0/444.0 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: docx2txt, requests, python-docx, pypdf, mypy-extensions, marshmallow, faiss-cpu, typing-inspect, openai, dataclasses-json, langchain-core, langchain-openai, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.101.0\n",
            "    Uninstalling openai-1.101.0:\n",
            "      Successfully uninstalled openai-1.101.0\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.74\n",
            "    Uninstalling langchain-core-0.3.74:\n",
            "      Successfully uninstalled langchain-core-0.3.74\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 docx2txt-0.9 faiss-cpu-1.12.0 langchain-community-0.3.29 langchain-core-0.3.75 langchain-openai-0.3.32 marshmallow-3.26.1 mypy-extensions-1.1.0 openai-1.102.0 pypdf-6.0.0 python-docx-1.2.0 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain-community langchain-openai faiss-cpu pypdf python-docx docx2txt openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "6Ud8KqkqkEgV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
        "\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "\n",
        "FOLDER_PATH = \"./all_filesz\"\n",
        "INDEX_PATH = \"./faiss_index\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwM4tAd-kKJv",
        "outputId": "fb5e3a4e-ad95-4cea-86e1-10e2996c77f3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "departments = [\"marketing\", \"policies\", \"technical_sops\", \"compliance_and_legal\"]\n",
        "\n",
        "all_files = {}\n",
        "\n",
        "for dept in departments:\n",
        "    folder_path = f\"/content/all_filesz/{dept}/*.docx\"\n",
        "    files = glob.glob(folder_path)\n",
        "    # Store with department name\n",
        "    all_files[dept] = files\n",
        "\n",
        "# Example: print each file with its department\n",
        "for dept, files in all_files.items():\n",
        "    for f in files:\n",
        "        print(f\"{os.path.basename(f)} --> {dept}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6NUWlK49EZ6",
        "outputId": "ab6d16e4-21a4-4829-c711-a42586f6e102"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q3_Content_Marketing_Strategy.docx --> marketing\n",
            "Brand_Guidelines_Handbook.docx --> marketing\n",
            "Product_Launch_Campaign_Report.docx --> marketing\n",
            "Employee_Leave_&_Absence_Policy.docx --> policies\n",
            "Remote_Work_&_Hybrid_Policy.docx --> policies\n",
            "Workplace_Code_of_Conduct.docx --> policies\n",
            "Production_Incident_Response_Runbook.docx --> technical_sops\n",
            "CI-CD_Deployment_Pipeline_Guide.docx --> technical_sops\n",
            "Backend_Service_Monitoring_SOP.docx --> technical_sops\n",
            "Contract_Review_Checklist (1).docx --> compliance_and_legal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FOLDER_PATH = \"/content/all_filesz\"\n",
        "INDEX_PATH = \"/content/faiss_index\"\n",
        "\n",
        "# Define department folders\n",
        "departments = [\"marketing\", \"policies\", \"technical_sops\", \"compliance_and_legal\"]\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "# ‚úÖ Load existing FAISS index if available\n",
        "if os.path.exists(f\"{INDEX_PATH}/index.faiss\"):\n",
        "    print(\"‚úÖ Loading existing FAISS index...\")\n",
        "    vectorstore = FAISS.load_local(INDEX_PATH, embedding, allow_dangerous_deserialization=True)\n",
        "\n",
        "else:\n",
        "    if not os.path.exists(FOLDER_PATH):\n",
        "        os.makedirs(FOLDER_PATH)\n",
        "        print(f\"üìÇ Created '{FOLDER_PATH}'. Please add department subfolders and rerun.\")\n",
        "        raise SystemExit\n",
        "\n",
        "    all_documents = []\n",
        "\n",
        "    # üîé Loop through each department folder\n",
        "    for dept in departments:\n",
        "        folder_path = f\"{FOLDER_PATH}/{dept}/*\"\n",
        "        files = glob.glob(folder_path)\n",
        "\n",
        "        for file in files:\n",
        "            file_name, ext = os.path.splitext(os.path.basename(file))\n",
        "            file_name = file_name.strip()\n",
        "\n",
        "            loader = None\n",
        "            if ext.lower() == \".pdf\":\n",
        "                loader = PyPDFLoader(file)\n",
        "            elif ext.lower() == \".docx\":\n",
        "                loader = Docx2txtLoader(file)\n",
        "            else:\n",
        "                continue  # skip unsupported formats\n",
        "\n",
        "            try:\n",
        "                documents = loader.load()\n",
        "\n",
        "                for i, doc in enumerate(documents, 1):\n",
        "                    doc.metadata.update({\n",
        "                        \"source_file\": os.path.basename(file),\n",
        "                        \"chunk_number\": i,\n",
        "                        \"page_number\": i,\n",
        "                        \"department\": dept,     # ‚úÖ taken from folder name\n",
        "                        \"title\": file_name      # ‚úÖ simple file name as title\n",
        "                    })\n",
        "\n",
        "                    # ‚úÖ Print debug info\n",
        "                    print(f\"üìÇ Department: {dept}\")\n",
        "                    print(f\"üìÑ Title: {file_name}\")\n",
        "                    print(f\"‚û°Ô∏è Source: {os.path.basename(file)}, Chunk: {i}\\n\")\n",
        "\n",
        "                all_documents.extend(documents)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Could not load {file}, skipping... ({e})\")\n",
        "                continue\n",
        "\n",
        "    # ‚ö° Create FAISS index from documents\n",
        "    vectorstore = FAISS.from_documents(all_documents, embedding)\n",
        "    vectorstore.save_local(INDEX_PATH)\n",
        "    print(\"‚úÖ New FAISS index created and saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99_Bu29gkMuB",
        "outputId": "c633d605-54b2-4cd1-f86e-833f332f1dc5"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loading existing FAISS index...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not all_documents:\n",
        "    print(f\"‚ö†Ô∏è No valid PDF/DOCX files found in '{FOLDER_PATH}'.\")\n",
        "    raise SystemExit\n",
        "\n",
        "os.makedirs(INDEX_PATH, exist_ok=True)\n",
        "vectorstore = FAISS.from_documents(all_documents, embedding)\n",
        "vectorstore.save_local(INDEX_PATH)\n",
        "print(\"‚úÖ FAISS index created and saved with department + title metadata!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPkbd8v7kQdT",
        "outputId": "e8175af3-cd2c-499f-f391-5d9bb668b663"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ FAISS index created and saved with department + title metadata!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = FAISS.load_local(INDEX_PATH, embedding, allow_dangerous_deserialization=True)\n",
        "\n",
        "# Get the total number of documents\n",
        "total_docs = len(vectorstore.docstore._dict)  # FAISS stores docs in docstore\n",
        "print(f\"Total documents in vectorstore: {total_docs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXQa2ScdkUfM",
        "outputId": "40f8b85b-85f7-4acf-8dab-6fa8d5b5feb4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total documents in vectorstore: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "chunks = splitter.split_documents(all_documents)\n",
        "\n",
        "\n",
        "vectorstore = FAISS.from_documents(chunks, embedding)\n",
        "vectorstore.save_local(INDEX_PATH)\n",
        "print(\"‚úÖ FAISS index created and saved!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pprWQ-XCkWbt",
        "outputId": "7b60125f-b38e-49b6-a1ce-a87e065c81f4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ FAISS index created and saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create retriever with filters inside\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_kwargs={\n",
        "        \"k\": 2,  # number of docs\n",
        "        \"filter\": {\n",
        "            \"$or\": [\n",
        "                {\"department\": \"marketing\"},\n",
        "                {\"department\": \"policies\"},\n",
        "                {\"department\": \"technical_sops\"},\n",
        "                {\"department\": \"compliance_and_legal\"}\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "# Now just call retriever without repeating filters\n",
        "results = retriever.get_relevant_documents(\"GDPR rules\")\n",
        "\n",
        "# Print nicely\n",
        "for doc in results:\n",
        "    print(f\"üìÇ Department: {doc.metadata.get('department')}\")\n",
        "    print(f\"üìÑ Title: {doc.metadata.get('title')}\")\n",
        "    print(f\"Content: {doc.page_content[:60]}...\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AsyP82lprcL",
        "outputId": "283a6dbe-9bd7-42b8-f0b5-8e6517c11a12"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Department: policies\n",
            "üìÑ Title: Remote Work & Hybrid Policy\n",
            "Content: Page 4\n",
            "\n",
            "Security Guidelines & VPN\n",
            "- All employees must use t...\n",
            "\n",
            "üìÇ Department: policies\n",
            "üìÑ Title: Workplace Code of Conduct\n",
            "Content: Workplace Code of Conduct\n",
            "\n",
            "Department: HR\n",
            "\n",
            "Region: Global\n",
            "\n",
            "V...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy_prompt = \"\"\"\n",
        "You are a Company Policy Assistant. Only answer questions using the information provided in the\n",
        "company's policy documents.\n",
        "If the answer is not found in the policy,\n",
        "say: I cannot find that information in the policy.\n",
        " Be concise, professional, and accurate.\"\"\""
      ],
      "metadata": {
        "id": "HueZGwjRpo3V"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Chatbot ready! Type 'exit' to stop.\")\n",
        "\n",
        "while True:\n",
        "    query = input(\"You: \")\n",
        "    if query.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    k = 1\n",
        "    relevant_docs = retriever.get_relevant_documents(query)[:k]\n",
        "\n",
        "    # Combine content and metadata for context\n",
        "    context_list = []\n",
        "    for i, doc in enumerate(relevant_docs, 1):\n",
        "        context_list.append(\n",
        "            f\"Chunk {i} Content:\\n{doc.page_content}\\nMetadata: {doc.metadata}\\n\"\n",
        "        )\n",
        "    context = \"\\n\".join(context_list)\n",
        "\n",
        "\n",
        "    print(\"\\n=== Retrieved Chunks ===\")\n",
        "    for doc in relevant_docs:\n",
        "        print(\"Content:\", doc.page_content[:40], \"...\")\n",
        "        print(\"Metadata:\", doc.metadata)\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": policy_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Context from documents:\\n{context}\\n\\nQuestion: {query}\"}\n",
        "    ]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4.1-nano\",\n",
        "        messages=messages,\n",
        "        max_tokens=300,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    print(\"Bot:\", response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIx7H-c-kcJr",
        "outputId": "70cea658-9afd-44ff-8abd-4c4457c2aaf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot ready! Type 'exit' to stop.\n",
            "You: what is Professional Conduct\n",
            "\n",
            "=== Retrieved Chunks ===\n",
            "Content: Workplace Code of Conduct\n",
            "\n",
            "Department: H ...\n",
            "Metadata: {'source': '/content/all_filesz/Workplace_Code_of_Conduct.docx', 'source_file': 'Workplace_Code_of_Conduct.docx', 'chunk_number': 1, 'page_number': 1, 'department': 'policies', 'title': 'Workplace Code of Conduct'}\n",
            "------------------------------\n",
            "Bot: Professional Conduct requires employees to uphold professionalism at all times, including respect for colleagues regardless of role or background, no tolerance for offensive, aggressive, or disruptive behavior, and maintaining confidentiality and data privacy.\n"
          ]
        }
      ]
    }
  ]
}