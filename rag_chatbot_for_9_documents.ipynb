{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARktfi7gtt1L",
        "outputId": "c07b88d7-2d47-48df-c0f6-4022a99e215b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.30-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.9-py3-none-any.whl.metadata (529 bytes)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.100.0)\n",
            "Collecting openai\n",
            "  Downloading openai-1.100.2-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.74)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.14)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.14.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain-community) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.30-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.0.0-py3-none-any.whl (310 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docx2txt-0.9-py3-none-any.whl (4.0 kB)\n",
            "Downloading openai-1.100.2-py3-none-any.whl (787 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m787.8/787.8 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: docx2txt, python-docx, pypdf, mypy-extensions, marshmallow, faiss-cpu, typing-inspect, openai, dataclasses-json, langchain-openai, langchain-community\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.100.0\n",
            "    Uninstalling openai-1.100.0:\n",
            "      Successfully uninstalled openai-1.100.0\n",
            "Successfully installed dataclasses-json-0.6.7 docx2txt-0.9 faiss-cpu-1.12.0 langchain-community-0.3.27 langchain-openai-0.3.30 marshmallow-3.26.1 mypy-extensions-1.1.0 openai-1.100.2 pypdf-6.0.0 python-docx-1.2.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain-community langchain-openai faiss-cpu pypdf python-docx docx2txt openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "DPU9d4k8sl9G"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from openai import OpenAI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csdHYU_-uWA4",
        "outputId": "29e9d656-754b-44ac-91b8-15271cd51f27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ··········\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
        "\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "\n",
        "FOLDER_PATH = \"./all_filesz\"\n",
        "INDEX_PATH = \"./faiss_index\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FOLDER_PATH = \"/content/all_filesz\"\n",
        "INDEX_PATH = \"/content/faiss_index\"\n",
        "\n",
        "\n",
        "# Production_Incident_Response_Runbook\n",
        "# CI-CD_Deployment_Pipeline_Guide\n",
        "# Department mapping\n",
        "departments = {\n",
        "    \"compliance_and_legal\": [\"Contract Review Checklist\", \"GDPR Compliance Guidelines\", \"HIPAA Data Handling Policy\"],\n",
        "    \"marketing\": [\"Brand Guidelines Handbook\", \"Product Launch Campaign Report\", \"Q3 Content Marketing Strategy\"],\n",
        "    \"policies\": [\"Employee Leave & Absence Policy\", \"Remote Work & Hybrid Policy\", \"Workplace Code of Conduct\"],\n",
        "    \"technical_sops\": [\"Backend Service Monitoring SOP\", \"CI CD Deployment Pipeline Guide\", \"Production Incident Response Runbook\"]\n",
        "}\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "# ✅ Load existing FAISS index\n",
        "if os.path.exists(f\"{INDEX_PATH}/index.faiss\"):\n",
        "    print(\"✅ Loading existing FAISS index...\")\n",
        "    vectorstore = FAISS.load_local(INDEX_PATH, embedding, allow_dangerous_deserialization=True)\n",
        "\n",
        "else:\n",
        "    if not os.path.exists(FOLDER_PATH):\n",
        "        os.makedirs(FOLDER_PATH)\n",
        "        print(f\"📂 Created '{FOLDER_PATH}'. Please add PDF/DOCX files and rerun.\")\n",
        "        raise SystemExit\n",
        "\n",
        "    all_documents = []\n",
        "\n",
        "    # 📥 Load documents\n",
        "    for file in os.listdir(FOLDER_PATH):\n",
        "        path = os.path.join(FOLDER_PATH, file)\n",
        "        file_name, ext = os.path.splitext(file)\n",
        "        file_name = file_name.strip()\n",
        "\n",
        "        matched_dept = \"\"\n",
        "        matched_title = file_name\n",
        "\n",
        "        # normalize file name (replace underscores & dashes with spaces, lowercase)\n",
        "        normalized_file = file_name.lower().replace(\"_\", \" \").replace(\"-\", \" \")\n",
        "\n",
        "        for dept, docs in departments.items():\n",
        "            for title in docs:\n",
        "                if title.lower() == normalized_file:\n",
        "                    matched_dept = dept\n",
        "                    matched_title = title\n",
        "                    break   # stop after first match\n",
        "\n",
        "        loader = None\n",
        "        if ext.lower() == \".pdf\":\n",
        "            loader = PyPDFLoader(path)\n",
        "        elif ext.lower() == \".docx\":\n",
        "            loader = Docx2txtLoader(path)\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            documents = loader.load()\n",
        "\n",
        "            for i, doc in enumerate(documents, 1):\n",
        "              doc.metadata.update({\n",
        "                  \"source_file\": file,\n",
        "                  \"chunk_number\": i,\n",
        "                  \"page_number\": i,\n",
        "                  \"department\": matched_dept,\n",
        "                  \"title\": matched_title\n",
        "              })\n",
        "            print(doc)\n",
        "            all_documents.extend(documents)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not load {file}, skipping... ({e})\")\n",
        "            continue\n",
        "\n",
        "        # 🏷️ Match department + title\n",
        "\n",
        "\n",
        "print(\"Department:\", matched_dept)\n",
        "print(\"Title:\", matched_title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7NUziYDf-GM",
        "outputId": "e67df93b-9994-4b06-f2f8-efca84ec2eb2"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Product Launch Campaign Report\n",
            "\n",
            "Department: Marketing\n",
            "\n",
            "Campaign ID: Q1-2025-Launch\n",
            "\n",
            "Region: North America\n",
            "\n",
            "Language: English\n",
            "\n",
            "Audience: Tech Decision Makers\n",
            "\n",
            "Page 1\n",
            "\n",
            "Campaign Overview\n",
            "Product: AI-Powered Analytics Suite v2.0\n",
            "Objective: Drive awareness and conversions from enterprise tech buyers.\n",
            "Channels: LinkedIn, Twitter, Email, and Google Ads.\n",
            "Campaign Duration: Jan 5 - Feb 29, 2025\n",
            "\n",
            "Page 2\n",
            "\n",
            "Performance Metrics\n",
            "- Impressions: 4.2M\n",
            "- CTR: 3.1%\n",
            "- MQLs Generated: 3,560\n",
            "- CAC: $71.25\n",
            "Top-performing asset: LinkedIn Carousel Ad (47% of leads).\n",
            "\n",
            "Page 3\n",
            "\n",
            "Audience Insights\n",
            "- 56% Traffic from USA, 22% from Canada, 14% from UK.\n",
            "- Most active job titles: CTO, Head of Data, Product Manager.\n",
            "- Peak engagement time: Weekdays 10–11 AM EST.\n",
            "Feedback indicated strong interest in integration with existing CRMs.\n",
            "\n",
            "Page 4\n",
            "\n",
            "Lessons & Recommendations\n",
            "- Invest more in video content for awareness phase.\n",
            "- Test gated vs. ungated whitepapers.\n",
            "- Automate retargeting for mid-funnel leads.\n",
            "Next phase: nurture journey with email + webinars.' metadata={'source': '/content/all_filesz/Product_Launch_Campaign_Report.docx', 'source_file': 'Product_Launch_Campaign_Report.docx', 'chunk_number': 1, 'page_number': 1, 'department': 'marketing', 'title': 'Product Launch Campaign Report'}\n",
            "page_content='Contract Review Checklist\n",
            "\n",
            "Department: Legal\n",
            "\n",
            "Jurisdiction: Global\n",
            "\n",
            "Compliance Area: Corporate\n",
            "\n",
            "Expiry Date: N/A\n",
            "\n",
            "Last Updated: 2025-04-10\n",
            "\n",
            "Page 1\n",
            "\n",
            "Purpose of This Checklist\n",
            "Used to evaluate all incoming and outgoing third-party contracts.\n",
            "Applicable to NDAs, MSAs, SOWs, vendor agreements, and licensing deals.\n",
            "Legal team must review contracts over $10,000 in value.\n",
            "\n",
            "Page 2\n",
            "\n",
            "Mandatory Clauses\n",
            "- Governing Law & Jurisdiction\n",
            "- Confidentiality & Non-Disclosure\n",
            "- Indemnity & Liability Caps\n",
            "- Termination & Force Majeure\n",
            "Every contract must have a clearly defined scope of work.\n",
            "\n",
            "Page 3\n",
            "\n",
            "Red Flags\n",
            "- Automatic renewal without notification.\n",
            "- Unlimited liability clauses.\n",
            "- Ambiguous terms or undefined KPIs.\n",
            "- Governing law outside of HQ jurisdiction.\n",
            "Always escalate contracts with non-standard clauses.\n",
            "\n",
            "Page 4\n",
            "\n",
            "Approval & Storage\n",
            "- All final contracts must be signed via DocuSign.\n",
            "- Contracts stored in the Legal Contract Repository (SharePoint).\n",
            "- Periodic audit every 6 months.\n",
            "Digital copies must be retained for a minimum of 7 years.' metadata={'source': '/content/all_filesz/Contract_Review_Checklist.docx', 'source_file': 'Contract_Review_Checklist.docx', 'chunk_number': 1, 'page_number': 1, 'department': 'compliance_and_legal', 'title': 'Contract Review Checklist'}\n",
            "⚠️ Could not load Employee_Leave_&_Absence_Policy.docx, skipping... (File is not a zip file)\n",
            "⚠️ Could not load Workplace_Code_of_Conduct.docx, skipping... (File is not a zip file)\n",
            "⚠️ Could not load Remote_Work_&_Hybrid_Policy.docx, skipping... (File is not a zip file)\n",
            "page_content='Brand Guidelines Handbook\n",
            "\n",
            "Department: Marketing\n",
            "\n",
            "Campaign ID: N/A\n",
            "\n",
            "Region: Global\n",
            "\n",
            "Language: English\n",
            "\n",
            "Audience: Internal Teams & Agencies\n",
            "\n",
            "Page 1\n",
            "\n",
            "Brand Identity\n",
            "Logo usage rules: clearspace, background contrast, and sizing.\n",
            "Primary colors: OakAI Blue (#004B87), Accent Green (#B4FFB0).\n",
            "Typography: Inter for web, Helvetica Neue for print.\n",
            "Brand voice: Confident, helpful, professional.\n",
            "\n",
            "Page 2\n",
            "\n",
            "Visual Assets\n",
            "- Only approved logo files may be used (SVG/PNG from Brand Portal).\n",
            "- Avoid adding shadow, bevel, or skew to logo.\n",
            "- Images must be high-res and relevant to technology or SaaS themes.\n",
            "- No stock photos with watermarks allowed.\n",
            "\n",
            "Page 3\n",
            "\n",
            "Content Guidelines\n",
            "- Tone: avoid jargon, use active voice, keep copy concise.\n",
            "- Use case stories should follow Problem > Solution > Outcome format.\n",
            "- Always include CTA in outbound content.\n",
            "- Use emoji only on social platforms, not in email/web copy.\n",
            "\n",
            "Page 4\n",
            "\n",
            "Approvals & Governance\n",
            "- All external creative must be reviewed by the Brand Committee.\n",
            "- Brand audits conducted quarterly.\n",
            "- Violations reported to the Director of Marketing.\n",
            "Always use latest templates from SharePoint repository.' metadata={'source': '/content/all_filesz/Brand_Guidelines_Handbook.docx', 'source_file': 'Brand_Guidelines_Handbook.docx', 'chunk_number': 1, 'page_number': 1, 'department': 'marketing', 'title': 'Brand Guidelines Handbook'}\n",
            "page_content='Production Incident Response Runbook\n",
            "\n",
            "Department: Engineering\n",
            "\n",
            "Team: SRE\n",
            "\n",
            "Platform: Kubernetes\n",
            "\n",
            "Product Version: All\n",
            "\n",
            "Last Updated: 2025-01-10\n",
            "\n",
            "Page 1\n",
            "\n",
            "Incident Classification\n",
            "- Sev 1: Critical outage, customer impact.\n",
            "- Sev 2: Partial outage or degraded performance.\n",
            "- Sev 3: Non-urgent, background errors.\n",
            "Incident must be declared in PagerDuty with clear severity label.\n",
            "Create ticket in Jira with timestamp and link to dashboards.\n",
            "\n",
            "Page 2\n",
            "\n",
            "First Responder Checklist\n",
            "1. Acknowledge alert in PagerDuty.\n",
            "2. Notify stakeholders in #incident Slack channel.\n",
            "3. Run `kubectl top pod` and check Grafana dashboards.\n",
            "4. Check logs in Kibana using the error code.\n",
            "Mitigation actions must be logged in real time.\n",
            "\n",
            "Page 3\n",
            "\n",
            "Escalation and War Room\n",
            "- Escalate to on-call engineer from impacted team.\n",
            "- Use Zoom bridge for ongoing updates.\n",
            "- Assign note-taker to document timeline.\n",
            "- Engineering Manager or SRE lead must send hourly updates.\n",
            "Communication must remain clear and focused.\n",
            "\n",
            "Page 4\n",
            "\n",
            "Postmortem Process\n",
            "- Root Cause Analysis (RCA) within 48 hours.\n",
            "- Action items must be tagged in Jira with owners and deadlines.\n",
            "- All Sev 1 & 2 incidents must be reviewed in monthly SRE review.\n",
            "Postmortems are stored in Confluence under Incident Reports.' metadata={'source': '/content/all_filesz/Production_Incident_Response_Runbook.docx', 'source_file': 'Production_Incident_Response_Runbook.docx', 'chunk_number': 1, 'page_number': 1, 'department': 'technical_sops', 'title': 'Production Incident Response Runbook'}\n",
            "page_content='CI/CD Deployment Pipeline Guide\n",
            "\n",
            "Department: Engineering\n",
            "\n",
            "Team: DevOps\n",
            "\n",
            "Platform: AWS\n",
            "\n",
            "Product Version: 2.3.1\n",
            "\n",
            "Last Updated: 2025-03-01\n",
            "\n",
            "Page 1\n",
            "\n",
            "Overview of CI/CD\n",
            "This document outlines the automated CI/CD workflow for deploying microservices.\n",
            "- Tools used: GitHub Actions, Docker, Kubernetes, Helm, ArgoCD.\n",
            "- Staging and production environments are deployed separately.\n",
            "- All pipelines are managed via YAML config files in the root directory.\n",
            "\n",
            "Page 2\n",
            "\n",
            "Pipeline Steps\n",
            "1. Linting & Static Code Analysis\n",
            "2. Unit & Integration Tests\n",
            "3. Build and Push Docker Image\n",
            "4. Deploy to Staging\n",
            "5. Manual Approval → Production\n",
            "Rollback:\n",
            "- ArgoCD auto-sync allows rollback within 30 minutes window.\n",
            "\n",
            "Page 3\n",
            "\n",
            "Secrets & Configuration Management\n",
            "- Use AWS Secrets Manager for storing credentials.\n",
            "- Helm values files must be encrypted using SOPS.\n",
            "- Environment-specific values are separated under `/helm/values/`.\n",
            "Validation:\n",
            "- Validate YAML with kubeval before deployment.\n",
            "- `helm template` preview required in PR review.\n",
            "\n",
            "Page 4\n",
            "\n",
            "Monitoring & Alerts\n",
            "- Prometheus and Grafana dashboards auto-deployed with every service.\n",
            "- Alerts configured via Alertmanager and PagerDuty.\n",
            "- Logs forwarded to ELK stack with structured logging (JSON).\n",
            "Responsibility Matrix:\n",
            "- Devs own testing & config.\n",
            "- DevOps own infrastructure & rollback.' metadata={'source': '/content/all_filesz/CI-CD_Deployment_Pipeline_Guide.docx', 'source_file': 'CI-CD_Deployment_Pipeline_Guide.docx', 'chunk_number': 1, 'page_number': 1, 'department': 'technical_sops', 'title': 'CI CD Deployment Pipeline Guide'}\n",
            "page_content='HIPAA Data Handling Policy\n",
            "\n",
            "Department: Legal\n",
            "\n",
            "Jurisdiction: USA\n",
            "\n",
            "Compliance Area: HIPAA\n",
            "\n",
            "Expiry Date: 2025-12-31\n",
            "\n",
            "Last Updated: 2023-10-15\n",
            "\n",
            "Page 1\n",
            "\n",
            "Overview of HIPAA\n",
            "HIPAA establishes rules for handling protected health information (PHI).\n",
            "Covers health plans, providers, and business associates.\n",
            "PHI includes medical records, billing information, and any identifiers.\n",
            "\n",
            "Page 2\n",
            "\n",
            "Access Controls\n",
            "- Role-based access: only minimum necessary access granted.\n",
            "- Two-factor authentication required for PHI systems.\n",
            "- Automatic logout from systems after 10 minutes idle.\n",
            "Access logs must be retained for 6 years.\n",
            "\n",
            "Page 3\n",
            "\n",
            "Transmission and Storage\n",
            "- PHI must be encrypted at rest and in transit.\n",
            "- Use TLS for all network communications.\n",
            "- No use of personal cloud storage for PHI.\n",
            "Backups must be stored securely offsite.\n",
            "\n",
            "Page 4\n",
            "\n",
            "Violations and Sanctions\n",
            "- Employees must report suspected breaches immediately.\n",
            "- Sanctions vary by severity and intent.\n",
            "- Civil penalties range from $100 to $50,000 per violation.\n",
            "Training on HIPAA must be completed annually.' metadata={'source': '/content/all_filesz/HIPAA_Data_Handling_Policy.docx', 'source_file': 'HIPAA_Data_Handling_Policy.docx', 'chunk_number': 1, 'page_number': 1, 'department': 'compliance_and_legal', 'title': 'HIPAA Data Handling Policy'}\n",
            "page_content='GDPR Compliance Guidelines\n",
            "\n",
            "Department: Legal\n",
            "\n",
            "Jurisdiction: EU\n",
            "\n",
            "Compliance Area: GDPR\n",
            "\n",
            "Expiry Date: 2026-12-31\n",
            "\n",
            "Last Updated: 2024-09-01\n",
            "\n",
            "Page 1\n",
            "\n",
            "Introduction to GDPR\n",
            "GDPR aims to protect personal data and privacy of EU citizens.\n",
            "- Applies to all data controllers and processors.\n",
            "- Covers any company processing EU resident data.\n",
            "Key Definitions:\n",
            "- Personal data, consent, data subject, data controller, data processor.\n",
            "\n",
            "Page 2\n",
            "\n",
            "User Consent Requirements\n",
            "- Consent must be freely given, specific, informed, and unambiguous.\n",
            "- No pre-ticked boxes or default opt-ins.\n",
            "- Consent must be recorded and auditable.\n",
            "Withdrawal of consent must be as easy as giving it.\n",
            "\n",
            "Page 3\n",
            "\n",
            "Rights of Data Subjects\n",
            "- Right to access, rectify, and erase their personal data.\n",
            "- Right to restrict processing and to data portability.\n",
            "- Right to object to profiling and automated decision-making.\n",
            "Response time: within 30 days of request receipt.\n",
            "\n",
            "Page 4\n",
            "\n",
            "Data Breach & Penalties\n",
            "- Notify supervisory authority within 72 hours.\n",
            "- Notify affected data subjects without undue delay.\n",
            "Penalties:\n",
            "- Up to €20M or 4% of global annual turnover.\n",
            "Documentation and DPIAs are mandatory for high-risk processing.' metadata={'source': '/content/all_filesz/GDPR_Compliance_Guidelines.docx', 'source_file': 'GDPR_Compliance_Guidelines.docx', 'chunk_number': 1, 'page_number': 1, 'department': 'compliance_and_legal', 'title': 'GDPR Compliance Guidelines'}\n",
            "page_content='Backend Service Monitoring SOP\n",
            "\n",
            "Department: Engineering\n",
            "\n",
            "Team: Backend\n",
            "\n",
            "Platform: Node.js + PostgreSQL\n",
            "\n",
            "Product Version: 1.8.7\n",
            "\n",
            "Last Updated: 2024-11-20\n",
            "\n",
            "Page 1\n",
            "\n",
            "Service Health Metrics\n",
            "All backend services expose metrics via /metrics (Prometheus format).\n",
            "Critical metrics include:\n",
            "- Request latency (p95, p99)\n",
            "- HTTP error rate (4xx, 5xx)\n",
            "- DB connection pool saturation\n",
            "All metrics must be visualized in Grafana.\n",
            "\n",
            "Page 2\n",
            "\n",
            "Alerting Configuration\n",
            "Alerts are triggered via Prometheus Alertmanager.\n",
            "- Alert thresholds defined in `alert_rules.yml`\n",
            "- Notification channels: Slack, Email, PagerDuty\n",
            "- Suppression rules applied during deployments\n",
            "Ensure alert fatigue is avoided by deduping noisy alerts.\n",
            "\n",
            "Page 3\n",
            "\n",
            "Log Management\n",
            "All logs must be structured (JSON format).\n",
            "Use Winston logger for Node.js apps.\n",
            "Logs are ingested into Elasticsearch via Fluent Bit.\n",
            "Log tags must include `service`, `env`, `trace_id` for tracing.\n",
            "\n",
            "Page 4\n",
            "\n",
            "Dashboards & Access\n",
            "- Grafana dashboards must be labeled per service and environment.\n",
            "- Use role-based access for dashboard editing.\n",
            "- Data retention: 90 days for metrics, 30 days for logs.\n",
            "- Access audits reviewed quarterly by Engineering Ops.' metadata={'source': '/content/all_filesz/Backend_Service_Monitoring_SOP.docx', 'source_file': 'Backend_Service_Monitoring_SOP.docx', 'chunk_number': 1, 'page_number': 1, 'department': 'technical_sops', 'title': 'Backend Service Monitoring SOP'}\n",
            "page_content='Q3 Content Marketing Strategy\n",
            "\n",
            "Department: Marketing\n",
            "\n",
            "Campaign ID: Q3-2025-CONTENT\n",
            "\n",
            "Region: APAC + EMEA\n",
            "\n",
            "Language: English\n",
            "\n",
            "Audience: Mid-Market SaaS Executives\n",
            "\n",
            "Page 1\n",
            "\n",
            "Strategy Overview\n",
            "Theme: \"Scaling with AI – Smart Growth for SaaS\"\n",
            "Objectives:\n",
            "- Build thought leadership through long-form blogs and video content.\n",
            "- Generate 2,500+ MQLs from inbound organic traffic.\n",
            "Core Markets: Singapore, UAE, Germany.\n",
            "\n",
            "Page 2\n",
            "\n",
            "Content Calendar Highlights\n",
            "- 2 Webinars (AI in Finance + AI for CX)\n",
            "- 6 Long-form SEO Blogs\n",
            "- 8 Short LinkedIn Posts per month\n",
            "- 1 Analyst Report Collaboration (IDC or G2)\n",
            "Publishing tools: HubSpot, Hootsuite, Notion.\n",
            "\n",
            "Page 3\n",
            "\n",
            "Target Persona Deep Dive\n",
            "- SaaS CTOs and Product Heads, $5–20M ARR\n",
            "- Common pain points: integration complexity, cost of AI adoption\n",
            "- Preferred channels: LinkedIn, TechCrunch, local tech newsletters\n",
            "Messaging pillars: Trust, Performance, Simplicity.\n",
            "\n",
            "Page 4\n",
            "\n",
            "KPIs & Measurement\n",
            "- Monthly Visitors Goal: 40,000+\n",
            "- Blog Conversion Rate: 1.8%\n",
            "- Webinar Attendance: 500+ per event\n",
            "Reporting via HubSpot dashboards, weekly review with performance team.' metadata={'source': '/content/all_filesz/Q3_Content_Marketing_Strategy.docx', 'source_file': 'Q3_Content_Marketing_Strategy.docx', 'chunk_number': 1, 'page_number': 1, 'department': 'marketing', 'title': 'Q3 Content Marketing Strategy'}\n",
            "Department: marketing\n",
            "Title: Q3 Content Marketing Strategy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not all_documents:\n",
        "    print(f\"⚠️ No valid PDF/DOCX files found in '{FOLDER_PATH}'.\")\n",
        "    raise SystemExit\n",
        "\n",
        "os.makedirs(INDEX_PATH, exist_ok=True)\n",
        "vectorstore = FAISS.from_documents(all_documents, embedding)\n",
        "vectorstore.save_local(INDEX_PATH)\n",
        "print(\"✅ FAISS index created and saved with department + title metadata!\")\n",
        ""
      ],
      "metadata": {
        "id": "bmm3fx-Ae86C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = FAISS.load_local(INDEX_PATH, embedding, allow_dangerous_deserialization=True)\n",
        "\n",
        "# Get the total number of documents\n",
        "total_docs = len(vectorstore.docstore._dict)  # FAISS stores docs in docstore\n",
        "print(f\"Total documents in vectorstore: {total_docs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCxsjtSDc3VZ",
        "outputId": "5a5c4ff5-3419-4f1a-b9aa-2fba17d20546"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total documents in vectorstore: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpEYDZo086ZP",
        "outputId": "65ab055a-64ea-4634-aab1-f2c7a4682fb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ FAISS index created and saved!\n"
          ]
        }
      ],
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "chunks = splitter.split_documents(all_documents)\n",
        "\n",
        "\n",
        "vectorstore = FAISS.from_documents(chunks, embedding)\n",
        "vectorstore.save_local(INDEX_PATH)\n",
        "print(\"✅ FAISS index created and saved!\")\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "\n",
        "policy_prompt = \"\"\"\n",
        "You are a Company Policy Assistant. Only answer questions using the information\n",
        "provided in the company's policy documents. If the answer is not found in the\n",
        "policy, say: \"I cannot find that information in the policy.\"\n",
        "Be concise, professional, and accurate.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gg9W-2oz9CaK"
      },
      "outputs": [],
      "source": [
        "results = retriever.get_relevant_documents(\n",
        "    \"GDPR rules\",\n",
        "    filter={\n",
        "        \"$or\": [\n",
        "            {\n",
        "                \"department\": \"marketing\",\n",
        "                \"title\": {\"$in\": [\n",
        "                    \"Brand Guidelines Handbook\",\n",
        "                    \"Product Launch Campaign Report\",\n",
        "                    \"Q3 Content Marketing Strategy\"\n",
        "                ]}\n",
        "            },\n",
        "            {\n",
        "                \"department\": \"policies\",\n",
        "                \"title\": {\"$in\": [\n",
        "                    \"Employee Leave & Absence Policy\",\n",
        "                    \"Remote Work & Hybrid Policy\",\n",
        "                    \"Workplace Code of Conduct\"\n",
        "                ]}\n",
        "            },\n",
        "            {\n",
        "                \"department\": \"technical sops\",\n",
        "                \"title\": {\"$in\": [\n",
        "                    \"Backend Service Monitoring SOP\",\n",
        "                    \"CI/CD Deployment Pipeline Guide\",\n",
        "                    \"Production Incident Response Runbook\"\n",
        "                ]}\n",
        "            },\n",
        "            {\n",
        "                \"department\": \"compliance_and_legal\",\n",
        "                \"title\": {\"$in\": [\n",
        "                    \"Contract Review Checklist\",\n",
        "                    \"GDPR Compliance Guidelines\",\n",
        "                    \"HIPAA Data Handling Policy\"\n",
        "                ]}\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        ")\n",
        "\n",
        "for doc in results:\n",
        "    print(f\"📂 Department: {doc.metadata.get('department')}\")\n",
        "    print(f\"📄 Title: {doc.metadata.get('title')}\")\n",
        "    print(f\"Content: {doc.page_content[:60]}...\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Chatbot ready! Type 'exit' to stop.\")\n",
        "\n",
        "while True:\n",
        "    query = input(\"You: \")\n",
        "    if query.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    k = 1\n",
        "    relevant_docs = retriever.get_relevant_documents(query)[:k]\n",
        "\n",
        "    # Combine content and metadata for context\n",
        "    context_list = []\n",
        "    for i, doc in enumerate(relevant_docs, 1):\n",
        "        context_list.append(\n",
        "            f\"Chunk {i} Content:\\n{doc.page_content}\\nMetadata: {doc.metadata}\\n\"\n",
        "        )\n",
        "    context = \"\\n\".join(context_list)\n",
        "    # print(\"\\n=== Context ===\")\n",
        "    # print(context)\n",
        "    # print(\"\\n==== End ====\")\n",
        "\n",
        "    print(\"\\n=== Retrieved Chunks ===\")\n",
        "    for doc in relevant_docs:\n",
        "        print(\"Content:\", doc.page_content[:40], \"...\")\n",
        "        print(\"Metadata:\", doc.metadata)\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": policy_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Context from documents:\\n{context}\\n\\nQuestion: {query}\"}\n",
        "    ]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4.1-nano\",\n",
        "        messages=messages,\n",
        "        max_tokens=300,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    print(\"Bot:\", response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWN6jJUIhSQB",
        "outputId": "d8d1302a-b214-4b44-aff3-d87f5acac113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot ready! Type 'exit' to stop.\n",
            "You: what is Strategy Overview\n",
            "\n",
            "=== Retrieved Chunks ===\n",
            "Content: Q3 Content Marketing Strategy\n",
            "\n",
            "Departmen ...\n",
            "Metadata: {'source': '/content/all_filesz/Q3_Content_Marketing_Strategy.docx', 'source_file': 'Q3_Content_Marketing_Strategy.docx', 'chunk_number': 1, 'page_number': 1, 'department': 'marketing', 'title': 'Q3 Content Marketing Strategy'}\n",
            "------------------------------\n",
            "Bot: The Strategy Overview is centered around the theme \"Scaling with AI – Smart Growth for SaaS,\" with objectives to build thought leadership through long-form blogs and video content, and to generate over 2,500 Marketing Qualified Leads (MQLs) from inbound organic traffic. The core markets are Singapore, UAE, and Germany.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dm-1Lg0xmW7t"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}