{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAw4iuwxjuHvc5+FEYINHh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ammad12345390/RAG_Chatbot-/blob/main/rag_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community langchain-openai faiss-cpu pypdf python-docx docx2txt openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWxwtcyv-x9P",
        "outputId": "f4783f88-8ced-4f83-a6f2-525cda490068"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.31-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.9-py3-none-any.whl.metadata (529 bytes)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.100.0)\n",
            "Collecting openai\n",
            "  Downloading openai-1.101.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.74)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.14)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.14.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain-community) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.31-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.0.0-py3-none-any.whl (310 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docx2txt-0.9-py3-none-any.whl (4.0 kB)\n",
            "Downloading openai-1.101.0-py3-none-any.whl (810 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m810.8/810.8 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: docx2txt, python-docx, pypdf, mypy-extensions, marshmallow, faiss-cpu, typing-inspect, openai, dataclasses-json, langchain-openai, langchain-community\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.100.0\n",
            "    Uninstalling openai-1.100.0:\n",
            "      Successfully uninstalled openai-1.100.0\n",
            "Successfully installed dataclasses-json-0.6.7 docx2txt-0.9 faiss-cpu-1.12.0 langchain-community-0.3.27 langchain-openai-0.3.31 marshmallow-3.26.1 mypy-extensions-1.1.0 openai-1.101.0 pypdf-6.0.0 python-docx-1.2.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "xIgWVK3G-uq2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
        "\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "\n",
        "FOLDER_PATH = \"./all_filesz\"\n",
        "INDEX_PATH = \"./faiss_index\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WuA3yKW-s9h",
        "outputId": "e86821bc-3ba5-41a9-e044-f725506e2427"
      },
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FOLDER_PATH = \"/content/all_filesz\"\n",
        "INDEX_PATH = \"/content/faiss_index\"\n",
        "\n",
        "departments = {\n",
        "    \"compliance_and_legal\": [\n",
        "        \"Contract Review Checklist\",\n",
        "        \"GDPR Compliance Guidelines\",\n",
        "        \"HIPAA Data Handling Policy\"\n",
        "    ],\n",
        "    \"marketing\": [\n",
        "        \"Brand Guidelines Handbook\",\n",
        "        \"Product Launch Campaign Report\",\n",
        "        \"Q3 Content Marketing Strategy\"\n",
        "    ],\n",
        "    \"policies\": [\n",
        "        \"Employee Leave & Absence Policy\",\n",
        "        \"Remote Work & Hybrid Policy\",\n",
        "        \"Workplace Code of Conduct\"\n",
        "    ],\n",
        "    \"technical_sops\": [\n",
        "        \"Backend Service Monitoring SOP\",\n",
        "        \"CI CD Deployment Pipeline Guide\",\n",
        "        \"Production Incident Response Runbook\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "# ✅ Load existing FAISS index\n",
        "if os.path.exists(f\"{INDEX_PATH}/index.faiss\"):\n",
        "    print(\"✅ Loading existing FAISS index...\")\n",
        "    vectorstore = FAISS.load_local(INDEX_PATH, embedding, allow_dangerous_deserialization=True)\n",
        "\n",
        "else:\n",
        "    if not os.path.exists(FOLDER_PATH):\n",
        "        os.makedirs(FOLDER_PATH)\n",
        "        print(f\"📂 Created '{FOLDER_PATH}'. Please add PDF/DOCX files and rerun.\")\n",
        "        raise SystemExit\n",
        "\n",
        "    all_documents = []\n",
        "\n",
        "    for file in os.listdir(FOLDER_PATH):\n",
        "        path = os.path.join(FOLDER_PATH, file)\n",
        "        file_name, ext = os.path.splitext(file)\n",
        "        file_name = file_name.strip()\n",
        "\n",
        "        matched_dept = \"\"\n",
        "        matched_title = file_name\n",
        "\n",
        "        # Normalize file name for matching\n",
        "        normalized_file = file_name.lower().replace(\"_\", \" \").replace(\"-\", \" \")\n",
        "\n",
        "        # 🔍 Match department and title\n",
        "        for dept, docs in departments.items():\n",
        "            for title in docs:\n",
        "                if title.lower() == normalized_file:\n",
        "                    matched_dept = dept\n",
        "                    matched_title = title\n",
        "                    break\n",
        "\n",
        "        loader = None\n",
        "        if ext.lower() == \".pdf\":\n",
        "            loader = PyPDFLoader(path)\n",
        "        elif ext.lower() == \".docx\":\n",
        "            loader = Docx2txtLoader(path)\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            documents = loader.load()\n",
        "\n",
        "            for i, doc in enumerate(documents, 1):\n",
        "                doc.metadata.update({\n",
        "                    \"source_file\": file,\n",
        "                    \"chunk_number\": i,\n",
        "                    \"page_number\": i,\n",
        "                    \"department\": matched_dept,\n",
        "                    \"title\": matched_title\n",
        "      })\n",
        "\n",
        "                # ✅ Print info for each document\n",
        "                print(f\"📂 Department: {matched_dept}\")\n",
        "                print(f\"📄 Title: {matched_title}\")\n",
        "                print(f\"➡️ Source: {file}, Chunk: {i}\\n\")\n",
        "\n",
        "            all_documents.extend(documents)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not load {file}, skipping... ({e})\")\n",
        "            continue\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCFMo888_muG",
        "outputId": "566767d5-7453-477d-88c0-e17880b6a938"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Department: marketing\n",
            "📄 Title: Q3 Content Marketing Strategy\n",
            "➡️ Source: Q3_Content_Marketing_Strategy.docx, Chunk: 1\n",
            "\n",
            "📂 Department: marketing\n",
            "📄 Title: Brand Guidelines Handbook\n",
            "➡️ Source: Brand_Guidelines_Handbook.docx, Chunk: 1\n",
            "\n",
            "📂 Department: technical_sops\n",
            "📄 Title: Backend Service Monitoring SOP\n",
            "➡️ Source: Backend_Service_Monitoring_SOP.docx, Chunk: 1\n",
            "\n",
            "📂 Department: compliance_and_legal\n",
            "📄 Title: Contract Review Checklist\n",
            "➡️ Source: Contract_Review_Checklist.docx, Chunk: 1\n",
            "\n",
            "📂 Department: compliance_and_legal\n",
            "📄 Title: GDPR Compliance Guidelines\n",
            "➡️ Source: GDPR_Compliance_Guidelines.docx, Chunk: 1\n",
            "\n",
            "📂 Department: marketing\n",
            "📄 Title: Product Launch Campaign Report\n",
            "➡️ Source: Product_Launch_Campaign_Report.docx, Chunk: 1\n",
            "\n",
            "📂 Department: \n",
            "📄 Title: Contract_Review_Checklist (1)\n",
            "➡️ Source: Contract_Review_Checklist (1).docx, Chunk: 1\n",
            "\n",
            "📂 Department: compliance_and_legal\n",
            "📄 Title: HIPAA Data Handling Policy\n",
            "➡️ Source: HIPAA_Data_Handling_Policy.docx, Chunk: 1\n",
            "\n",
            "📂 Department: technical_sops\n",
            "📄 Title: CI CD Deployment Pipeline Guide\n",
            "➡️ Source: CI-CD_Deployment_Pipeline_Guide.docx, Chunk: 1\n",
            "\n",
            "📂 Department: technical_sops\n",
            "📄 Title: Production Incident Response Runbook\n",
            "➡️ Source: Production_Incident_Response_Runbook.docx, Chunk: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not all_documents:\n",
        "    print(f\"⚠️ No valid PDF/DOCX files found in '{FOLDER_PATH}'.\")\n",
        "    raise SystemExit\n",
        "\n",
        "os.makedirs(INDEX_PATH, exist_ok=True)\n",
        "vectorstore = FAISS.from_documents(all_documents, embedding)\n",
        "vectorstore.save_local(INDEX_PATH)\n",
        "print(\"✅ FAISS index created and saved with department + title metadata!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBOYHkym_pVG",
        "outputId": "93af7b04-0c33-44be-fe9d-ddcfddefa0cc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ FAISS index created and saved with department + title metadata!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = FAISS.load_local(INDEX_PATH, embedding, allow_dangerous_deserialization=True)\n",
        "\n",
        "# Get the total number of documents\n",
        "total_docs = len(vectorstore.docstore._dict)  # FAISS stores docs in docstore\n",
        "print(f\"Total documents in vectorstore: {total_docs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnBOTriv_rpu",
        "outputId": "44ee4110-2eb6-4bff-f570-136d0445def3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total documents in vectorstore: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "chunks = splitter.split_documents(all_documents)\n",
        "\n",
        "\n",
        "vectorstore = FAISS.from_documents(chunks, embedding)\n",
        "vectorstore.save_local(INDEX_PATH)\n",
        "print(\"✅ FAISS index created and saved!\")\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "\n",
        "policy_prompt = \"\"\"\n",
        "You are a Company Policy Assistant. Only answer questions using the information\n",
        "provided in the company's policy documents. If the answer is not found in the\n",
        "policy, say: \"I cannot find that information in the policy.\"\n",
        "Be concise, professional, and accurate.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFvbujeb-g1p",
        "outputId": "85822df7-b5dd-4f5a-ae30-ac10955b9f37"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ FAISS index created and saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = retriever.get_relevant_documents(\n",
        "    \"GDPR rules\",\n",
        "    filter={\n",
        "        \"$or\": [\n",
        "            {\n",
        "                \"department\": \"marketing\",\n",
        "                \"title\": {\"$in\": [\n",
        "                    \"Brand Guidelines Handbook\",\n",
        "                    \"Product Launch Campaign Report\",\n",
        "                    \"Q3 Content Marketing Strategy\"\n",
        "                ]}\n",
        "            },\n",
        "            {\n",
        "                \"department\": \"policies\",\n",
        "                \"title\": {\"$in\": [\n",
        "                    \"Employee Leave & Absence Policy\",\n",
        "                    \"Remote Work & Hybrid Policy\",\n",
        "                    \"Workplace Code of Conduct\"\n",
        "                ]}\n",
        "            },\n",
        "            {\n",
        "                \"department\": \"technical sops\",\n",
        "                \"title\": {\"$in\": [\n",
        "                    \"Backend Service Monitoring SOP\",\n",
        "                    \"CI/CD Deployment Pipeline Guide\",\n",
        "                    \"Production Incident Response Runbook\"\n",
        "                ]}\n",
        "            },\n",
        "            {\n",
        "                \"department\": \"compliance_and_legal\",\n",
        "                \"title\": {\"$in\": [\n",
        "                    \"Contract Review Checklist\",\n",
        "                    \"GDPR Compliance Guidelines\",\n",
        "                    \"HIPAA Data Handling Policy\"\n",
        "                ]}\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        ")\n",
        "\n",
        "for doc in results:\n",
        "    print(f\"📂 Department: {doc.metadata.get('department')}\")\n",
        "    print(f\"📄 Title: {doc.metadata.get('title')}\")\n",
        "    print(f\"Content: {doc.page_content[:60]}...\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1UOReXy-dRY",
        "outputId": "cecbe021-6688-436f-9776-775604eef400"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1758526566.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  results = retriever.get_relevant_documents(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Department: compliance_and_legal\n",
            "📄 Title: GDPR Compliance Guidelines\n",
            "Content: GDPR Compliance Guidelines\n",
            "\n",
            "Department: Legal\n",
            "\n",
            "Jurisdiction:...\n",
            "\n",
            "📂 Department: compliance_and_legal\n",
            "📄 Title: GDPR Compliance Guidelines\n",
            "Content: Page 2\n",
            "\n",
            "User Consent Requirements\n",
            "- Consent must be freely g...\n",
            "\n",
            "📂 Department: compliance_and_legal\n",
            "📄 Title: GDPR Compliance Guidelines\n",
            "Content: Page 4\n",
            "\n",
            "Data Breach & Penalties\n",
            "- Notify supervisory authori...\n",
            "\n",
            "📂 Department: compliance_and_legal\n",
            "📄 Title: HIPAA Data Handling Policy\n",
            "Content: HIPAA Data Handling Policy\n",
            "\n",
            "Department: Legal\n",
            "\n",
            "Jurisdiction:...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Chatbot ready! Type 'exit' to stop.\")\n",
        "\n",
        "while True:\n",
        "    query = input(\"You: \")\n",
        "    if query.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    k = 1\n",
        "    relevant_docs = retriever.get_relevant_documents(query)[:k]\n",
        "\n",
        "    # Combine content and metadata for context\n",
        "    context_list = []\n",
        "    for i, doc in enumerate(relevant_docs, 1):\n",
        "        context_list.append(\n",
        "            f\"Chunk {i} Content:\\n{doc.page_content}\\nMetadata: {doc.metadata}\\n\"\n",
        "        )\n",
        "    context = \"\\n\".join(context_list)\n",
        "\n",
        "\n",
        "    print(\"\\n=== Retrieved Chunks ===\")\n",
        "    for doc in relevant_docs:\n",
        "        print(\"Content:\", doc.page_content[:40], \"...\")\n",
        "        print(\"Metadata:\", doc.metadata)\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": policy_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Context from documents:\\n{context}\\n\\nQuestion: {query}\"}\n",
        "    ]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4.1-nano\",\n",
        "        messages=messages,\n",
        "        max_tokens=300,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    print(\"Bot:\", response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dWilj_n-ZJp",
        "outputId": "48dd6bc0-f034-45b5-a216-4b2cd474b43a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot ready! Type 'exit' to stop.\n",
            "You: what is Overview of CI/CD\n",
            "\n",
            "=== Retrieved Chunks ===\n",
            "Content: CI/CD Deployment Pipeline Guide\n",
            "\n",
            "Departm ...\n",
            "Metadata: {'source': '/content/all_filesz/CI-CD_Deployment_Pipeline_Guide.docx', 'source_file': 'CI-CD_Deployment_Pipeline_Guide.docx', 'chunk_number': 1, 'page_number': 1, 'department': 'technical_sops', 'title': 'CI CD Deployment Pipeline Guide'}\n",
            "------------------------------\n",
            "Bot: The overview of CI/CD describes the automated workflow for deploying microservices using tools such as GitHub Actions, Docker, Kubernetes, Helm, and ArgoCD. It states that staging and production environments are deployed separately and that all pipelines are managed via YAML configuration files in the root directory.\n"
          ]
        }
      ]
    }
  ]
}