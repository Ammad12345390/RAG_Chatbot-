{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAw4iuwxjuHvc5+FEYINHh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ammad12345390/RAG_Chatbot-/blob/main/rag_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community langchain-openai faiss-cpu pypdf python-docx docx2txt openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWxwtcyv-x9P",
        "outputId": "ca81532f-b1a8-4ca4-d95e-6c14b0588de5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.28)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (0.3.32)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.0.0)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: docx2txt in /usr/local/lib/python3.12/dist-packages (0.9)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.102.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.74 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.74)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.14)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.14.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<1.0.0,>=0.3.27->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain-community) (1.33)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.74->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "xIgWVK3G-uq2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
        "\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "\n",
        "FOLDER_PATH = \"./all_filesz\"\n",
        "INDEX_PATH = \"./faiss_index\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WuA3yKW-s9h",
        "outputId": "2a610643-3192-43ba-f31b-980d0a2c3f59"
      },
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FOLDER_PATH = \"/content/all_filesz\"\n",
        "INDEX_PATH = \"/content/faiss_index\"\n",
        "\n",
        "departments = {\n",
        "    \"compliance_and_legal\": [\n",
        "        \"Contract Review Checklist\",\n",
        "        \"GDPR Compliance Guidelines\",\n",
        "        \"HIPAA Data Handling Policy\"\n",
        "    ],\n",
        "    \"marketing\": [\n",
        "        \"Brand Guidelines Handbook\",\n",
        "        \"Product Launch Campaign Report\",\n",
        "        \"Q3 Content Marketing Strategy\"\n",
        "    ],\n",
        "    \"policies\": [\n",
        "        \"Employee Leave & Absence Policy\",\n",
        "        \"Remote Work & Hybrid Policy\",\n",
        "        \"Workplace Code of Conduct\"\n",
        "    ],\n",
        "    \"technical_sops\": [\n",
        "        \"Backend Service Monitoring SOP\",\n",
        "        \"CI CD Deployment Pipeline Guide\",\n",
        "        \"Production Incident Response Runbook\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "# ‚úÖ Load existing FAISS index\n",
        "if os.path.exists(f\"{INDEX_PATH}/index.faiss\"):\n",
        "    print(\"‚úÖ Loading existing FAISS index...\")\n",
        "    vectorstore = FAISS.load_local(INDEX_PATH, embedding, allow_dangerous_deserialization=True)\n",
        "\n",
        "else:\n",
        "    if not os.path.exists(FOLDER_PATH):\n",
        "        os.makedirs(FOLDER_PATH)\n",
        "        print(f\"üìÇ Created '{FOLDER_PATH}'. Please add PDF/DOCX files and rerun.\")\n",
        "        raise SystemExit\n",
        "\n",
        "    all_documents = []\n",
        "\n",
        "    for file in os.listdir(FOLDER_PATH):\n",
        "        path = os.path.join(FOLDER_PATH, file)\n",
        "        file_name, ext = os.path.splitext(file)\n",
        "        file_name = file_name.strip()\n",
        "\n",
        "        matched_dept = \"\"\n",
        "        matched_title = file_name\n",
        "\n",
        "        # Normalize file name for matching\n",
        "        normalized_file = file_name.lower().replace(\"_\", \" \").replace(\"-\", \" \")\n",
        "\n",
        "        # üîç Match department and title\n",
        "        for dept, docs in departments.items():\n",
        "            for title in docs:\n",
        "                if title.lower() == normalized_file:\n",
        "                    matched_dept = dept\n",
        "                    matched_title = title\n",
        "                    break\n",
        "\n",
        "        loader = None\n",
        "        if ext.lower() == \".pdf\":\n",
        "            loader = PyPDFLoader(path)\n",
        "        elif ext.lower() == \".docx\":\n",
        "            loader = Docx2txtLoader(path)\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            documents = loader.load()\n",
        "\n",
        "            for i, doc in enumerate(documents, 1):\n",
        "                doc.metadata.update({\n",
        "                    \"source_file\": file,\n",
        "                    \"chunk_number\": i,\n",
        "                    \"page_number\": i,\n",
        "                    \"department\": matched_dept,\n",
        "                    \"title\": matched_title\n",
        "      })\n",
        "\n",
        "                # ‚úÖ Print info for each document\n",
        "                print(f\"üìÇ Department: {matched_dept}\")\n",
        "                print(f\"üìÑ Title: {matched_title}\")\n",
        "                print(f\"‚û°Ô∏è Source: {file}, Chunk: {i}\\n\")\n",
        "\n",
        "            all_documents.extend(documents)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Could not load {file}, skipping... ({e})\")\n",
        "            continue\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCFMo888_muG",
        "outputId": "c300c771-b316-4c98-8f04-e2f3f1cb409b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Department: marketing\n",
            "üìÑ Title: Brand Guidelines Handbook\n",
            "‚û°Ô∏è Source: Brand_Guidelines_Handbook.docx, Chunk: 1\n",
            "\n",
            "üìÇ Department: policies\n",
            "üìÑ Title: Workplace Code of Conduct\n",
            "‚û°Ô∏è Source: Workplace_Code_of_Conduct.docx, Chunk: 1\n",
            "\n",
            "üìÇ Department: marketing\n",
            "üìÑ Title: Q3 Content Marketing Strategy\n",
            "‚û°Ô∏è Source: Q3_Content_Marketing_Strategy.docx, Chunk: 1\n",
            "\n",
            "üìÇ Department: technical_sops\n",
            "üìÑ Title: Backend Service Monitoring SOP\n",
            "‚û°Ô∏è Source: Backend_Service_Monitoring_SOP.docx, Chunk: 1\n",
            "\n",
            "üìÇ Department: technical_sops\n",
            "üìÑ Title: Production Incident Response Runbook\n",
            "‚û°Ô∏è Source: Production_Incident_Response_Runbook.docx, Chunk: 1\n",
            "\n",
            "üìÇ Department: technical_sops\n",
            "üìÑ Title: CI CD Deployment Pipeline Guide\n",
            "‚û°Ô∏è Source: CI-CD_Deployment_Pipeline_Guide.docx, Chunk: 1\n",
            "\n",
            "üìÇ Department: \n",
            "üìÑ Title: Contract_Review_Checklist (1)\n",
            "‚û°Ô∏è Source: Contract_Review_Checklist (1).docx, Chunk: 1\n",
            "\n",
            "üìÇ Department: policies\n",
            "üìÑ Title: Remote Work & Hybrid Policy\n",
            "‚û°Ô∏è Source: Remote_Work_&_Hybrid_Policy.docx, Chunk: 1\n",
            "\n",
            "üìÇ Department: policies\n",
            "üìÑ Title: Employee Leave & Absence Policy\n",
            "‚û°Ô∏è Source: Employee_Leave_&_Absence_Policy.docx, Chunk: 1\n",
            "\n",
            "üìÇ Department: marketing\n",
            "üìÑ Title: Product Launch Campaign Report\n",
            "‚û°Ô∏è Source: Product_Launch_Campaign_Report.docx, Chunk: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not all_documents:\n",
        "    print(f\"‚ö†Ô∏è No valid PDF/DOCX files found in '{FOLDER_PATH}'.\")\n",
        "    raise SystemExit\n",
        "\n",
        "os.makedirs(INDEX_PATH, exist_ok=True)\n",
        "vectorstore = FAISS.from_documents(all_documents, embedding)\n",
        "vectorstore.save_local(INDEX_PATH)\n",
        "print(\"‚úÖ FAISS index created and saved with department + title metadata!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBOYHkym_pVG",
        "outputId": "889b073b-c904-43a9-a376-c6194fc3a362"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ FAISS index created and saved with department + title metadata!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = FAISS.load_local(INDEX_PATH, embedding, allow_dangerous_deserialization=True)\n",
        "\n",
        "# Get the total number of documents\n",
        "total_docs = len(vectorstore.docstore._dict)  # FAISS stores docs in docstore\n",
        "print(f\"Total documents in vectorstore: {total_docs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnBOTriv_rpu",
        "outputId": "ba40a641-79e4-48d4-d78d-1b45fe0f439c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total documents in vectorstore: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "chunks = splitter.split_documents(all_documents)\n",
        "\n",
        "\n",
        "vectorstore = FAISS.from_documents(chunks, embedding)\n",
        "vectorstore.save_local(INDEX_PATH)\n",
        "print(\"‚úÖ FAISS index created and saved!\")\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "\n",
        "policy_prompt = \"\"\"\n",
        "You are a Company Policy Assistant. Only answer questions using the information\n",
        "provided in the company's policy documents. If the answer is not found in the\n",
        "policy, say: \"I cannot find that information in the policy.\"\n",
        "Be concise, professional, and accurate.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFvbujeb-g1p",
        "outputId": "06a1575e-47e4-493f-c376-3c4f92c70a52"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ FAISS index created and saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = retriever.get_relevant_documents(\n",
        "    \"GDPR rules\",\n",
        "    filter={\n",
        "        \"$or\": [\n",
        "            {\n",
        "                \"department\": \"marketing\",\n",
        "                \"title\": {\"$in\": [\n",
        "                    \"Brand Guidelines Handbook\",\n",
        "                    \"Product Launch Campaign Report\",\n",
        "                    \"Q3 Content Marketing Strategy\"\n",
        "                ]}\n",
        "            },\n",
        "            {\n",
        "                \"department\": \"policies\",\n",
        "                \"title\": {\"$in\": [\n",
        "                    \"Employee Leave & Absence Policy\",\n",
        "                    \"Remote Work & Hybrid Policy\",\n",
        "                    \"Workplace Code of Conduct\"\n",
        "                ]}\n",
        "            },\n",
        "            {\n",
        "                \"department\": \"technical sops\",\n",
        "                \"title\": {\"$in\": [\n",
        "                    \"Backend Service Monitoring SOP\",\n",
        "                    \"CI/CD Deployment Pipeline Guide\",\n",
        "                    \"Production Incident Response Runbook\"\n",
        "                ]}\n",
        "            },\n",
        "            {\n",
        "                \"department\": \"compliance_and_legal\",\n",
        "                \"title\": {\"$in\": [\n",
        "                    \"Contract Review Checklist\",\n",
        "                    \"GDPR Compliance Guidelines\",\n",
        "                    \"HIPAA Data Handling Policy\"\n",
        "                ]}\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        ")\n",
        "\n",
        "for doc in results:\n",
        "    print(f\"üìÇ Department: {doc.metadata.get('department')}\")\n",
        "    print(f\"üìÑ Title: {doc.metadata.get('title')}\")\n",
        "    print(f\"Content: {doc.page_content[:60]}...\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1UOReXy-dRY",
        "outputId": "ba4636b0-164c-48b7-9cd9-b36472e6b981"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1758526566.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  results = retriever.get_relevant_documents(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Department: policies\n",
            "üìÑ Title: Remote Work & Hybrid Policy\n",
            "Content: Page 4\n",
            "\n",
            "Security Guidelines & VPN\n",
            "- All employees must use t...\n",
            "\n",
            "üìÇ Department: policies\n",
            "üìÑ Title: Workplace Code of Conduct\n",
            "Content: Workplace Code of Conduct\n",
            "\n",
            "Department: HR\n",
            "\n",
            "Region: Global\n",
            "\n",
            "V...\n",
            "\n",
            "üìÇ Department: marketing\n",
            "üìÑ Title: Brand Guidelines Handbook\n",
            "Content: Page 3\n",
            "\n",
            "Content Guidelines\n",
            "- Tone: avoid jargon, use active ...\n",
            "\n",
            "üìÇ Department: policies\n",
            "üìÑ Title: Workplace Code of Conduct\n",
            "Content: Page 3\n",
            "\n",
            "Conflict of Interest & Gifts\n",
            "- Employees must disclo...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Chatbot ready! Type 'exit' to stop.\")\n",
        "\n",
        "while True:\n",
        "    query = input(\"You: \")\n",
        "    if query.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    k = 1\n",
        "    relevant_docs = retriever.get_relevant_documents(query)[:k]\n",
        "\n",
        "    # Combine content and metadata for context\n",
        "    context_list = []\n",
        "    for i, doc in enumerate(relevant_docs, 1):\n",
        "        context_list.append(\n",
        "            f\"Chunk {i} Content:\\n{doc.page_content}\\nMetadata: {doc.metadata}\\n\"\n",
        "        )\n",
        "    context = \"\\n\".join(context_list)\n",
        "\n",
        "\n",
        "    print(\"\\n=== Retrieved Chunks ===\")\n",
        "    for doc in relevant_docs:\n",
        "        print(\"Content:\", doc.page_content[:40], \"...\")\n",
        "        print(\"Metadata:\", doc.metadata)\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": policy_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Context from documents:\\n{context}\\n\\nQuestion: {query}\"}\n",
        "    ]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4.1-nano\",\n",
        "        messages=messages,\n",
        "        max_tokens=300,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    print(\"Bot:\", response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "Rlr42T-m1er2",
        "outputId": "bde77225-b50d-4b9f-c2a3-c0020a50ee28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot ready! Type 'exit' to stop.\n",
            "You: what is Overview of CI/CD\n",
            "\n",
            "=== Retrieved Chunks ===\n",
            "Content: CI/CD Deployment Pipeline Guide\n",
            "\n",
            "Departm ...\n",
            "Metadata: {'source': '/content/all_filesz/CI-CD_Deployment_Pipeline_Guide.docx', 'source_file': 'CI-CD_Deployment_Pipeline_Guide.docx', 'chunk_number': 1, 'page_number': 1, 'department': 'technical_sops', 'title': 'CI CD Deployment Pipeline Guide'}\n",
            "------------------------------\n",
            "Bot: The overview of CI/CD in the document outlines the automated workflow for deploying microservices, including the tools used (GitHub Actions, Docker, Kubernetes, Helm, ArgoCD), the separation of staging and production environments, and the management of pipelines via YAML configuration files in the root directory.\n"
          ]
        }
      ]
    }
  ]
}