{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnQhOgecpuo/aAPFXdpCKu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ammad12345390/RAG_Chatbot-/blob/main/rag_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community langchain-openai faiss-cpu pypdf python-docx docx2txt openai"
      ],
      "metadata": {
        "id": "Mu3Cusrxh1Qg",
        "outputId": "61c5df6c-194f-473c-9560-bddb85944615",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.28)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (0.3.32)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.0.0)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: docx2txt in /usr/local/lib/python3.12/dist-packages (0.9)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.102.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.74 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.74)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.14)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.14.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<1.0.0,>=0.3.27->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain-community) (1.33)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.74->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "7akHwWOTh3d3"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
        "\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "\n",
        "FOLDER_PATH = \"./all_filesz\"\n",
        "INDEX_PATH = \"./faiss_index\""
      ],
      "metadata": {
        "id": "mnl_-QzGh6gf",
        "outputId": "c262e02d-00d1-4282-9d38-730f6fe5d3a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "a = glob.glob('/content/all_filesz/*docx')\n",
        "a"
      ],
      "metadata": {
        "id": "DMXsgR60qPOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "from collections import defaultdict\n",
        "import shutil\n",
        "\n",
        "# Folder containing all your files\n",
        "base_folder = \"/content/all_filesz\"\n",
        "files = glob.glob(os.path.join(base_folder, \"*.docx\"))\n",
        "\n",
        "# Define department keywords (lowercase)\n",
        "department_keywords = {\n",
        "    \"compliance_and_legal\": [\"contract\", \"gdpr\", \"hipaa\", \"legal\"],\n",
        "    \"marketing\": [\"brand\", \"product\", \"campaign\", \"marketing\"],\n",
        "    \"policies\": [\"leave\", \"absence\", \"workplace\", \"remote\", \"policy\"],\n",
        "    \"technical_sops\": [\"backend\", \"deployment\", \"pipeline\", \"incident\", \"ci-cd\", \"ci_cd\"]\n",
        "}\n",
        "\n",
        "departments = defaultdict(list)\n",
        "\n",
        "# Create department folders if not exist\n",
        "for dept in department_keywords.keys():\n",
        "    os.makedirs(os.path.join(base_folder, dept), exist_ok=True)\n",
        "os.makedirs(os.path.join(base_folder, \"uncategorized\"), exist_ok=True)\n",
        "\n",
        "# Categorize and move files\n",
        "for file_path in files:\n",
        "    file_name = os.path.basename(file_path).lower()\n",
        "    assigned = False\n",
        "\n",
        "    for dept, keywords in department_keywords.items():\n",
        "        if any(keyword in file_name for keyword in keywords):\n",
        "            departments[dept].append(file_name)\n",
        "            # Move file to department folder\n",
        "            shutil.move(file_path, os.path.join(base_folder, dept, os.path.basename(file_path)))\n",
        "            assigned = True\n",
        "            break\n",
        "\n",
        "    if not assigned:\n",
        "        departments[\"uncategorized\"].append(file_name)\n",
        "        shutil.move(file_path, os.path.join(base_folder, \"uncategorized\", os.path.basename(file_path)))\n",
        "\n",
        "# Print result\n",
        "for dept, docs in departments.items():\n",
        "    print(f\"{dept}: {docs}\\n\")\n"
      ],
      "metadata": {
        "id": "HMntfqXqiDYr",
        "outputId": "87427ae7-002a-48df-c876-a7dbca58d2d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "marketing: ['brand_guidelines_handbook.docx', 'q3_content_marketing_strategy.docx', 'production_incident_response_runbook.docx', 'product_launch_campaign_report.docx']\n",
            "\n",
            "policies: ['workplace_code_of_conduct.docx', 'remote_work_&_hybrid_policy.docx', 'employee_leave_&_absence_policy.docx']\n",
            "\n",
            "technical_sops: ['backend_service_monitoring_sop.docx', 'ci-cd_deployment_pipeline_guide.docx']\n",
            "\n",
            "compliance_and_legal: ['contract_review_checklist (1).docx']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.document_loaders import PyPDFLoader, Docx2txtLoader\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "FOLDER_PATH = \"/content/all_filesz\"\n",
        "INDEX_PATH = \"/content/faiss_index\"\n",
        "\n",
        "# Define department keywords for automatic matching\n",
        "department_keywords = {\n",
        "    \"compliance_and_legal\": [\"contract\", \"gdpr\", \"hipaa\", \"legal\"],\n",
        "    \"marketing\": [\"brand\", \"product\", \"campaign\", \"marketing\", \"q3\"],\n",
        "    \"policies\": [\"leave\", \"absence\", \"workplace\", \"remote\", \"policy\"],\n",
        "    \"technical_sops\": [\"backend\", \"deployment\", \"pipeline\", \"incident\", \"ci-cd\", \"ci_cd\"]\n",
        "}\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "# Load existing FAISS index if it exists\n",
        "if os.path.exists(os.path.join(INDEX_PATH, \"index.faiss\")):\n",
        "    print(\"‚úÖ Loading existing FAISS index...\")\n",
        "    vectorstore = FAISS.load_local(INDEX_PATH, embedding, allow_dangerous_deserialization=True)\n",
        "\n",
        "else:\n",
        "    if not os.path.exists(FOLDER_PATH):\n",
        "        os.makedirs(FOLDER_PATH)\n",
        "        print(f\"üìÇ Created '{FOLDER_PATH}'. Please add PDF/DOCX files and rerun.\")\n",
        "        raise SystemExit\n",
        "\n",
        "    all_documents = []\n",
        "\n",
        "    # Walk through all files, including subfolders\n",
        "    for root, dirs, files in os.walk(FOLDER_PATH):\n",
        "        for file in files:\n",
        "            path = os.path.join(root, file)\n",
        "            file_name, ext = os.path.splitext(file)\n",
        "            normalized_file = file_name.lower().replace(\"_\", \" \").replace(\"-\", \" \")\n",
        "\n",
        "            # Automatic department matching using keywords\n",
        "            matched_dept = \"uncategorized\"\n",
        "            for dept, keywords in department_keywords.items():\n",
        "                if any(keyword in normalized_file for keyword in keywords):\n",
        "                    matched_dept = dept\n",
        "                    break\n",
        "\n",
        "            # Load document\n",
        "            loader = None\n",
        "            if ext.lower() == \".pdf\":\n",
        "                loader = PyPDFLoader(path)\n",
        "            elif ext.lower() == \".docx\":\n",
        "                loader = Docx2txtLoader(path)\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                documents = loader.load()\n",
        "                for i, doc in enumerate(documents, 1):\n",
        "                    doc.metadata.update({\n",
        "                        \"source_file\": file,\n",
        "                        \"chunk_number\": i,\n",
        "                        \"page_number\": i,\n",
        "                        \"department\": matched_dept,\n",
        "                        \"title\": file_name\n",
        "                    })\n",
        "                    print(f\"üìÇ Department: {matched_dept}\")\n",
        "                    print(f\"üìÑ Title: {file_name}\")\n",
        "                    print(f\"‚û°Ô∏è Source: {file}, Chunk: {i}\\n\")\n",
        "\n",
        "                all_documents.extend(documents)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Could not load {file}, skipping... ({e})\")\n",
        "                continue\n",
        "\n",
        "    # Build and save FAISS index\n",
        "    if all_documents:\n",
        "        vectorstore = FAISS.from_documents(all_documents, embedding)\n",
        "        os.makedirs(INDEX_PATH, exist_ok=True)\n",
        "        vectorstore.save_local(INDEX_PATH)\n",
        "        print(f\"‚úÖ FAISS index saved to {INDEX_PATH}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No documents loaded, FAISS index not created.\")\n"
      ],
      "metadata": {
        "id": "csuFsK0Vhh3Z",
        "outputId": "d032bc39-d132-4840-b07e-991a95722c8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loading existing FAISS index...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not all_documents:\n",
        "    print(f\"‚ö†Ô∏è No valid PDF/DOCX files found in '{FOLDER_PATH}'.\")\n",
        "    raise SystemExit\n",
        "\n",
        "os.makedirs(INDEX_PATH, exist_ok=True)\n",
        "vectorstore = FAISS.from_documents(all_documents, embedding)\n",
        "vectorstore.save_local(INDEX_PATH)\n",
        "print(\"‚úÖ FAISS index created and saved with department + title metadata!\")"
      ],
      "metadata": {
        "id": "BaKnN2KwiTXF",
        "outputId": "1bda3724-4771-4368-e882-d728d8f570eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ FAISS index created and saved with department + title metadata!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = FAISS.load_local(INDEX_PATH, embedding, allow_dangerous_deserialization=True)\n",
        "\n",
        "\n",
        "total_docs = len(vectorstore.docstore._dict)\n",
        "print(f\"Total documents in vectorstore: {total_docs}\")"
      ],
      "metadata": {
        "id": "5qF9WgaSiWTg",
        "outputId": "05ce063a-7631-4946-db39-c127f6d050c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total documents in vectorstore: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "chunks = splitter.split_documents(all_documents)\n",
        "\n",
        "\n",
        "vectorstore = FAISS.from_documents(chunks, embedding)\n",
        "vectorstore.save_local(INDEX_PATH)\n",
        "print(\"‚úÖ FAISS index created and saved!\")\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "\n",
        "policy_prompt = \"\"\"\n",
        "You are a Company Policy Assistant. Only answer questions using the information\n",
        "provided in the company's policy documents. If the answer is not found in the\n",
        "policy, say: \"I cannot find that information in the policy.\"\n",
        "Be concise, professional, and accurate.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "8QEsrNmIiZ-h",
        "outputId": "fd91ca50-9b9e-4343-b334-2c6c3a8aab0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ FAISS index created and saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"GDPR rules\"\n",
        "departments = [\"marketing\", \"policies\", \"technical_sops\", \"compliance_and_legal\"]\n",
        "k = 2  # top 2 relevant documents per department\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for dept in departments:\n",
        "    results = vectorstore.similarity_search(\n",
        "        query,\n",
        "        k=k,\n",
        "        filter={\"department\": dept}  # filter by department\n",
        "    )\n",
        "    all_results.extend(results)\n",
        "\n",
        "# Sort all results by relevance score if your vector store provides it\n",
        "# (Some vector stores return results already sorted by similarity)\n",
        "\n",
        "# Print only the relevant results\n",
        "for doc in all_results:\n",
        "    print(f\"üìÇ Department: {doc.metadata.get('department')}\")\n",
        "    print(f\"üìÑ Title: {doc.metadata.get('title')}\")\n",
        "    print(f\"Content: {doc.page_content[:100]}...\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "kV5746daicmM",
        "outputId": "4ef0382a-49b9-499b-e047-47597ab6f075",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Department: marketing\n",
            "üìÑ Title: Brand Guidelines Handbook\n",
            "Content: Page 3\n",
            "\n",
            "Content Guidelines\n",
            "- Tone: avoid jargon, use active voice, keep copy concise.\n",
            "- Use case sto...\n",
            "\n",
            "üìÇ Department: marketing\n",
            "üìÑ Title: Brand Guidelines Handbook\n",
            "Content: Brand Guidelines Handbook\n",
            "\n",
            "Department: Marketing\n",
            "\n",
            "Campaign ID: N/A\n",
            "\n",
            "Region: Global\n",
            "\n",
            "Language: Englis...\n",
            "\n",
            "üìÇ Department: policies\n",
            "üìÑ Title: Remote Work & Hybrid Policy\n",
            "Content: Page 4\n",
            "\n",
            "Security Guidelines & VPN\n",
            "- All employees must use the company VPN when accessing internal s...\n",
            "\n",
            "üìÇ Department: policies\n",
            "üìÑ Title: Workplace Code of Conduct\n",
            "Content: Workplace Code of Conduct\n",
            "\n",
            "Department: HR\n",
            "\n",
            "Region: Global\n",
            "\n",
            "Version: v4.0\n",
            "\n",
            "Last Updated: 2025-02-20\n",
            "\n",
            "...\n",
            "\n",
            "üìÇ Department: technical_sops\n",
            "üìÑ Title: Backend Service Monitoring SOP\n",
            "Content: Page 3\n",
            "\n",
            "Log Management\n",
            "All logs must be structured (JSON format).\n",
            "Use Winston logger for Node.js app...\n",
            "\n",
            "üìÇ Department: technical_sops\n",
            "üìÑ Title: CI CD Deployment Pipeline Guide\n",
            "Content: Page 4\n",
            "\n",
            "Monitoring & Alerts\n",
            "- Prometheus and Grafana dashboards auto-deployed with every service.\n",
            "- ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Chatbot ready! Type 'exit' to stop.\")\n",
        "\n",
        "while True:\n",
        "    query = input(\"You: \")\n",
        "    if query.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    k = 1\n",
        "    relevant_docs = retriever.get_relevant_documents(query)[:k]\n",
        "\n",
        "    # Combine content and metadata for context\n",
        "    context_list = []\n",
        "    for i, doc in enumerate(relevant_docs, 1):\n",
        "        context_list.append(\n",
        "            f\"Chunk {i} Content:\\n{doc.page_content}\\nMetadata: {doc.metadata}\\n\"\n",
        "        )\n",
        "    context = \"\\n\".join(context_list)\n",
        "\n",
        "\n",
        "    print(\"\\n=== Retrieved Chunks ===\")\n",
        "    for doc in relevant_docs:\n",
        "        print(\"Content:\", doc.page_content[:40], \"...\")\n",
        "        print(\"Metadata:\", doc.metadata)\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": policy_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Context from documents:\\n{context}\\n\\nQuestion: {query}\"}\n",
        "    ]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4.1-nano\",\n",
        "        messages=messages,\n",
        "        max_tokens=300,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    print(\"Bot:\", response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "QcFL_LILie-r",
        "outputId": "eb795845-502c-4c80-a82e-41dfd16b604b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot ready! Type 'exit' to stop.\n",
            "\n",
            "=== Retrieved Chunks ===\n",
            "Content: Workplace Code of Conduct\n",
            "\n",
            "Department: H ...\n",
            "Metadata: {'source': '/content/all_filesz/Workplace_Code_of_Conduct.docx', 'source_file': 'Workplace_Code_of_Conduct.docx', 'chunk_number': 1, 'page_number': 1, 'department': 'policies', 'title': 'Workplace Code of Conduct'}\n",
            "------------------------------\n",
            "Bot: Professional Conduct refers to employees upholding professionalism at all times, which includes respecting colleagues regardless of role or background, having no tolerance for offensive, aggressive, or disruptive behavior, and maintaining confidentiality and data privacy at all times.\n"
          ]
        }
      ]
    }
  ]
}