{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAw4iuwxjuHvc5+FEYINHh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ammad12345390/RAG_Chatbot-/blob/main/rag_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community langchain-openai faiss-cpu pypdf python-docx docx2txt openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWxwtcyv-x9P",
        "outputId": "ca81532f-b1a8-4ca4-d95e-6c14b0588de5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.28)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (0.3.32)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.0.0)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: docx2txt in /usr/local/lib/python3.12/dist-packages (0.9)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.102.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.74 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.74)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.14)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.14.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<1.0.0,>=0.3.27->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain-community) (1.33)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.74->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "xIgWVK3G-uq2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
        "\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "\n",
        "FOLDER_PATH = \"./all_filesz\"\n",
        "INDEX_PATH = \"./faiss_index\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WuA3yKW-s9h",
        "outputId": "2a610643-3192-43ba-f31b-980d0a2c3f59"
      },
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FOLDER_PATH = \"/content/all_filesz\"\n",
        "INDEX_PATH = \"/content/faiss_index\"\n",
        "\n",
        "departments = {\n",
        "    \"compliance_and_legal\": [\n",
        "        \"Contract Review Checklist\",\n",
        "        \"GDPR Compliance Guidelines\",\n",
        "        \"HIPAA Data Handling Policy\"\n",
        "    ],\n",
        "    \"marketing\": [\n",
        "        \"Brand Guidelines Handbook\",\n",
        "        \"Product Launch Campaign Report\",\n",
        "        \"Q3 Content Marketing Strategy\"\n",
        "    ],\n",
        "    \"policies\": [\n",
        "        \"Employee Leave & Absence Policy\",\n",
        "        \"Remote Work & Hybrid Policy\",\n",
        "        \"Workplace Code of Conduct\"\n",
        "    ],\n",
        "    \"technical_sops\": [\n",
        "        \"Backend Service Monitoring SOP\",\n",
        "        \"CI CD Deployment Pipeline Guide\",\n",
        "        \"Production Incident Response Runbook\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "# ✅ Load existing FAISS index\n",
        "if os.path.exists(f\"{INDEX_PATH}/index.faiss\"):\n",
        "    print(\"✅ Loading existing FAISS index...\")\n",
        "    vectorstore = FAISS.load_local(INDEX_PATH, embedding, allow_dangerous_deserialization=True)\n",
        "\n",
        "else:\n",
        "    if not os.path.exists(FOLDER_PATH):\n",
        "        os.makedirs(FOLDER_PATH)\n",
        "        print(f\"📂 Created '{FOLDER_PATH}'. Please add PDF/DOCX files and rerun.\")\n",
        "        raise SystemExit\n",
        "\n",
        "    all_documents = []\n",
        "\n",
        "    for file in os.listdir(FOLDER_PATH):\n",
        "        path = os.path.join(FOLDER_PATH, file)\n",
        "        file_name, ext = os.path.splitext(file)\n",
        "        file_name = file_name.strip()\n",
        "\n",
        "        matched_dept = \"\"\n",
        "        matched_title = file_name\n",
        "\n",
        "        # Normalize file name for matching\n",
        "        normalized_file = file_name.lower().replace(\"_\", \" \").replace(\"-\", \" \")\n",
        "\n",
        "        # 🔍 Match department and title\n",
        "        for dept, docs in departments.items():\n",
        "            for title in docs:\n",
        "                if title.lower() == normalized_file:\n",
        "                    matched_dept = dept\n",
        "                    matched_title = title\n",
        "                    break\n",
        "\n",
        "        loader = None\n",
        "        if ext.lower() == \".pdf\":\n",
        "            loader = PyPDFLoader(path)\n",
        "        elif ext.lower() == \".docx\":\n",
        "            loader = Docx2txtLoader(path)\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            documents = loader.load()\n",
        "\n",
        "            for i, doc in enumerate(documents, 1):\n",
        "                doc.metadata.update({\n",
        "                    \"source_file\": file,\n",
        "                    \"chunk_number\": i,\n",
        "                    \"page_number\": i,\n",
        "                    \"department\": matched_dept,\n",
        "                    \"title\": matched_title\n",
        "      })\n",
        "\n",
        "                # ✅ Print info for each document\n",
        "                print(f\"📂 Department: {matched_dept}\")\n",
        "                print(f\"📄 Title: {matched_title}\")\n",
        "                print(f\"➡️ Source: {file}, Chunk: {i}\\n\")\n",
        "\n",
        "            all_documents.extend(documents)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not load {file}, skipping... ({e})\")\n",
        "            continue\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCFMo888_muG",
        "outputId": "c300c771-b316-4c98-8f04-e2f3f1cb409b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Department: marketing\n",
            "📄 Title: Brand Guidelines Handbook\n",
            "➡️ Source: Brand_Guidelines_Handbook.docx, Chunk: 1\n",
            "\n",
            "📂 Department: policies\n",
            "📄 Title: Workplace Code of Conduct\n",
            "➡️ Source: Workplace_Code_of_Conduct.docx, Chunk: 1\n",
            "\n",
            "📂 Department: marketing\n",
            "📄 Title: Q3 Content Marketing Strategy\n",
            "➡️ Source: Q3_Content_Marketing_Strategy.docx, Chunk: 1\n",
            "\n",
            "📂 Department: technical_sops\n",
            "📄 Title: Backend Service Monitoring SOP\n",
            "➡️ Source: Backend_Service_Monitoring_SOP.docx, Chunk: 1\n",
            "\n",
            "📂 Department: technical_sops\n",
            "📄 Title: Production Incident Response Runbook\n",
            "➡️ Source: Production_Incident_Response_Runbook.docx, Chunk: 1\n",
            "\n",
            "📂 Department: technical_sops\n",
            "📄 Title: CI CD Deployment Pipeline Guide\n",
            "➡️ Source: CI-CD_Deployment_Pipeline_Guide.docx, Chunk: 1\n",
            "\n",
            "📂 Department: \n",
            "📄 Title: Contract_Review_Checklist (1)\n",
            "➡️ Source: Contract_Review_Checklist (1).docx, Chunk: 1\n",
            "\n",
            "📂 Department: policies\n",
            "📄 Title: Remote Work & Hybrid Policy\n",
            "➡️ Source: Remote_Work_&_Hybrid_Policy.docx, Chunk: 1\n",
            "\n",
            "📂 Department: policies\n",
            "📄 Title: Employee Leave & Absence Policy\n",
            "➡️ Source: Employee_Leave_&_Absence_Policy.docx, Chunk: 1\n",
            "\n",
            "📂 Department: marketing\n",
            "📄 Title: Product Launch Campaign Report\n",
            "➡️ Source: Product_Launch_Campaign_Report.docx, Chunk: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not all_documents:\n",
        "    print(f\"⚠️ No valid PDF/DOCX files found in '{FOLDER_PATH}'.\")\n",
        "    raise SystemExit\n",
        "\n",
        "os.makedirs(INDEX_PATH, exist_ok=True)\n",
        "vectorstore = FAISS.from_documents(all_documents, embedding)\n",
        "vectorstore.save_local(INDEX_PATH)\n",
        "print(\"✅ FAISS index created and saved with department + title metadata!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBOYHkym_pVG",
        "outputId": "889b073b-c904-43a9-a376-c6194fc3a362"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ FAISS index created and saved with department + title metadata!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = FAISS.load_local(INDEX_PATH, embedding, allow_dangerous_deserialization=True)\n",
        "\n",
        "# Get the total number of documents\n",
        "total_docs = len(vectorstore.docstore._dict)  # FAISS stores docs in docstore\n",
        "print(f\"Total documents in vectorstore: {total_docs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnBOTriv_rpu",
        "outputId": "ba40a641-79e4-48d4-d78d-1b45fe0f439c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total documents in vectorstore: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "chunks = splitter.split_documents(all_documents)\n",
        "\n",
        "\n",
        "vectorstore = FAISS.from_documents(chunks, embedding)\n",
        "vectorstore.save_local(INDEX_PATH)\n",
        "print(\"✅ FAISS index created and saved!\")\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "\n",
        "policy_prompt = \"\"\"\n",
        "You are a Company Policy Assistant. Only answer questions using the information\n",
        "provided in the company's policy documents. If the answer is not found in the\n",
        "policy, say: \"I cannot find that information in the policy.\"\n",
        "Be concise, professional, and accurate.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFvbujeb-g1p",
        "outputId": "06a1575e-47e4-493f-c376-3c4f92c70a52"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ FAISS index created and saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = retriever.get_relevant_documents(\n",
        "    \"GDPR rules\",\n",
        "    filter={\n",
        "        \"$or\": [\n",
        "            {\n",
        "                \"department\": \"marketing\",\n",
        "                \"title\": {\"$in\": [\n",
        "                    \"Brand Guidelines Handbook\",\n",
        "                    \"Product Launch Campaign Report\",\n",
        "                    \"Q3 Content Marketing Strategy\"\n",
        "                ]}\n",
        "            },\n",
        "            {\n",
        "                \"department\": \"policies\",\n",
        "                \"title\": {\"$in\": [\n",
        "                    \"Employee Leave & Absence Policy\",\n",
        "                    \"Remote Work & Hybrid Policy\",\n",
        "                    \"Workplace Code of Conduct\"\n",
        "                ]}\n",
        "            },\n",
        "            {\n",
        "                \"department\": \"technical sops\",\n",
        "                \"title\": {\"$in\": [\n",
        "                    \"Backend Service Monitoring SOP\",\n",
        "                    \"CI/CD Deployment Pipeline Guide\",\n",
        "                    \"Production Incident Response Runbook\"\n",
        "                ]}\n",
        "            },\n",
        "            {\n",
        "                \"department\": \"compliance_and_legal\",\n",
        "                \"title\": {\"$in\": [\n",
        "                    \"Contract Review Checklist\",\n",
        "                    \"GDPR Compliance Guidelines\",\n",
        "                    \"HIPAA Data Handling Policy\"\n",
        "                ]}\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        ")\n",
        "\n",
        "for doc in results:\n",
        "    print(f\"📂 Department: {doc.metadata.get('department')}\")\n",
        "    print(f\"📄 Title: {doc.metadata.get('title')}\")\n",
        "    print(f\"Content: {doc.page_content[:60]}...\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1UOReXy-dRY",
        "outputId": "ba4636b0-164c-48b7-9cd9-b36472e6b981"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1758526566.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  results = retriever.get_relevant_documents(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Department: policies\n",
            "📄 Title: Remote Work & Hybrid Policy\n",
            "Content: Page 4\n",
            "\n",
            "Security Guidelines & VPN\n",
            "- All employees must use t...\n",
            "\n",
            "📂 Department: policies\n",
            "📄 Title: Workplace Code of Conduct\n",
            "Content: Workplace Code of Conduct\n",
            "\n",
            "Department: HR\n",
            "\n",
            "Region: Global\n",
            "\n",
            "V...\n",
            "\n",
            "📂 Department: marketing\n",
            "📄 Title: Brand Guidelines Handbook\n",
            "Content: Page 3\n",
            "\n",
            "Content Guidelines\n",
            "- Tone: avoid jargon, use active ...\n",
            "\n",
            "📂 Department: policies\n",
            "📄 Title: Workplace Code of Conduct\n",
            "Content: Page 3\n",
            "\n",
            "Conflict of Interest & Gifts\n",
            "- Employees must disclo...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Chatbot ready! Type 'exit' to stop.\")\n",
        "\n",
        "while True:\n",
        "    query = input(\"You: \")\n",
        "    if query.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    k = 1\n",
        "    relevant_docs = retriever.get_relevant_documents(query)[:k]\n",
        "\n",
        "    # Combine content and metadata for context\n",
        "    context_list = []\n",
        "    for i, doc in enumerate(relevant_docs, 1):\n",
        "        context_list.append(\n",
        "            f\"Chunk {i} Content:\\n{doc.page_content}\\nMetadata: {doc.metadata}\\n\"\n",
        "        )\n",
        "    context = \"\\n\".join(context_list)\n",
        "\n",
        "\n",
        "    print(\"\\n=== Retrieved Chunks ===\")\n",
        "    for doc in relevant_docs:\n",
        "        print(\"Content:\", doc.page_content[:40], \"...\")\n",
        "        print(\"Metadata:\", doc.metadata)\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": policy_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Context from documents:\\n{context}\\n\\nQuestion: {query}\"}\n",
        "    ]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4.1-nano\",\n",
        "        messages=messages,\n",
        "        max_tokens=300,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    print(\"Bot:\", response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "Rlr42T-m1er2",
        "outputId": "bde77225-b50d-4b9f-c2a3-c0020a50ee28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot ready! Type 'exit' to stop.\n",
            "You: what is Overview of CI/CD\n",
            "\n",
            "=== Retrieved Chunks ===\n",
            "Content: CI/CD Deployment Pipeline Guide\n",
            "\n",
            "Departm ...\n",
            "Metadata: {'source': '/content/all_filesz/CI-CD_Deployment_Pipeline_Guide.docx', 'source_file': 'CI-CD_Deployment_Pipeline_Guide.docx', 'chunk_number': 1, 'page_number': 1, 'department': 'technical_sops', 'title': 'CI CD Deployment Pipeline Guide'}\n",
            "------------------------------\n",
            "Bot: The overview of CI/CD in the document outlines the automated workflow for deploying microservices, including the tools used (GitHub Actions, Docker, Kubernetes, Helm, ArgoCD), the separation of staging and production environments, and the management of pipelines via YAML configuration files in the root directory.\n"
          ]
        }
      ]
    }
  ]
}